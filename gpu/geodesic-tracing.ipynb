{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italian-chamber",
   "metadata": {},
   "source": [
    "# A naive look: GPU accelerated geodesic tracing\n",
    "The work here is the research into first learning the [CUDA.jl](https://juliagpu.gitlab.io/CUDA.jl/) library and applying it to my Black Hole rendering project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monthly-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-turkish",
   "metadata": {},
   "source": [
    "## CPU Function\n",
    "Here is a modified version of the function I am aiming to run on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sitting-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renderdisk (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function truncator(px) \n",
    "    floor(min(max(0, px), 255))\n",
    "end\n",
    "\n",
    "function renderdisk(\n",
    "    α::Float64,\n",
    "    β::Float64,\n",
    "    geodesics,\n",
    "    ;\n",
    "    height::Int=720,\n",
    "    width::Int=1080,\n",
    "    fov_index::Int=200,\n",
    "    rinit::Function=zeros\n",
    "    )\n",
    "\n",
    "    data = rinit(Float64, (height, width, 3))\n",
    "\n",
    "    mid = width ÷ 2 # integer division\n",
    "    mid2 = height ÷ 2 # integer division\n",
    "    n = length(geodesics)\n",
    "\n",
    "    for x in (-mid):(mid - 1)\n",
    "        \n",
    "        col = rinit(Float64, (height, 3))\n",
    "        \n",
    "        for y in 1:height-1\n",
    "            β2 = atan(y-mid2, x) # atan2\n",
    "            r = sqrt(x^2 + (y-mid2)^2) # distance from middle of image\n",
    "            i = convert(\n",
    "                Int,\n",
    "                floor(1 + r / fov_index * n)\n",
    "            )\n",
    "            if i < n\n",
    "                col[height-y, :] .= intersection(geodesics[i], α, β + β2)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        data[:, x + mid + 1, :] .= truncator.(col)\n",
    "    end\n",
    "\n",
    "    sum(data)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-express",
   "metadata": {},
   "source": [
    "For now, I'm going to simplify the intersection problem to just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporated-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intersection (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function intersection(g, α, β)\n",
    "    return 100 * β * α\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-virginia",
   "metadata": {},
   "source": [
    "Now we can get a starting benchmark for execution by generating some sample arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "center-celtic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodesics = map(\n",
    "    _ -> ones(Float64, (3, 300)),\n",
    "    1:1000\n",
    ")\n",
    "geodesics[1][:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-record",
   "metadata": {},
   "source": [
    "for which the above function takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vocational-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14.567 ms (2162 allocations: 35.68 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "837741.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime renderdisk(π/100, 0.0, $geodesics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-suffering",
   "metadata": {},
   "source": [
    "## Naive GPU\n",
    "CUDA.jl implements an `AbstractArray` type `CuArray`, which is the basis for interchanging memory with the GPU and the host machine.\n",
    "\n",
    "The place to then start is by uploading all of the `Array` types into `CuArrays`, and then change the return init function `rinit` passed to `renderdisk` to be `CUDA.zeros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "large-partnership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector{CuArray{Float64, 2}} (alias for Array{CuArray{Float64, 2}, 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function upload(a::AbstractArray{<:Array})\n",
    "    # upload\n",
    "    [CUDA.CuArray(i) for i in a]\n",
    "end\n",
    "\n",
    "gpugeo = upload(geodesics)\n",
    "typeof(gpugeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-roots",
   "metadata": {},
   "source": [
    "Thus we get the naive benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reasonable-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  253.622 ms (1154653 allocations: 64.41 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "837741.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function renderdiskgpu(α, β, gpugeo)\n",
    "    renderdisk(α, β, gpugeo; rinit=CUDA.zeros)\n",
    "end\n",
    "\n",
    "@btime renderdiskgpu(π/100, 0.0, $gpugeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-prophet",
   "metadata": {},
   "source": [
    "which is significantly slower. This is to be expected, we are taking zero advantage of the GPU's hardware, and instead just running sloppy single threaded CPU code on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-traveler",
   "metadata": {},
   "source": [
    "## Using broadcasts\n",
    "The GPU is very adapted to doing *embarassingly parallel* problems, in which very little scalar multiplication happens. The first thing we ought to do is assess where our GPU code is, and what computations we are exercising.\n",
    "\n",
    "In the `renderdisk` function, we loop over `x` and `y` indices of a large zero array, representing the output image.\n",
    "\n",
    "Each loop is independent, so lets try and isolate this stage and speed it up. To make the computation costly, we'll compute the quantities `β2` and `r`.\n",
    "\n",
    "The approach when writing this is to remember\n",
    "- each loop iteration is slow, but we can do very many simultaneously\n",
    "- write to maximize number of unnested loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brutal-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  51.198 ms (4 allocations: 35.60 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10166.387038500514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function imager(rinit::Function; height=720, width=1080)\n",
    "    data = rinit(Float64, (height, width, 3))\n",
    "    \n",
    "    h_mid = height ÷ 2\n",
    "    w_mid = width ÷ 2\n",
    "        \n",
    "    values = map(\n",
    "        (i) -> begin \n",
    "            # find position\n",
    "            z = (i-1) ÷ (width * height)\n",
    "            y = (i-1 - z*width*height) ÷ width\n",
    "            x = i - ( z*width*height + y * width ) - w_mid\n",
    "            y += 1\n",
    "            z += 1\n",
    "            \n",
    "            β2 = atan(y - h_mid, x) # atan2\n",
    "            r = sqrt(x^2 + (y - h_mid)^2)\n",
    "            \n",
    "            β2\n",
    "        end,\n",
    "        1:length(data)\n",
    "    )\n",
    "    \n",
    "    sum(values)\n",
    "end\n",
    "\n",
    "@btime imager(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nervous-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  49.822 ms (15 allocations: 17.80 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10166.387038500514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime imager(CUDA.zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-actress",
   "metadata": {},
   "source": [
    "But we haven't yet taken advantage of the CUDA api. For this, we want to call the CUDA variant of `map`, which takes a CuArray as the second argument. Since we are just counting incrementally for `i`, we can pre-assemble this information using something like\n",
    "```julia\n",
    "reshape(collect(Float64, 1:width*height*3), (height, width, 3))\n",
    "```\n",
    "\n",
    "Implementing this new version, modifying `rinit` to be a `::Type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daily-honor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imager (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function imager(rinit::Type; height=720, width=1080)\n",
    "    data = reshape(\n",
    "        rinit(collect(Float64, 1:width*height*3)),\n",
    "        (height, width, 3)\n",
    "    )\n",
    "    \n",
    "    h_mid = height ÷ 2\n",
    "    w_mid = width ÷ 2\n",
    "        \n",
    "    values = map(\n",
    "        (i) -> begin \n",
    "            # find position\n",
    "            z = (i-1) ÷ (width * height)\n",
    "            y = (i-1 - z*width*height) ÷ width\n",
    "            x = i - ( z*width*height + y * width ) - w_mid\n",
    "            y += 1\n",
    "            z += 1\n",
    "            \n",
    "            β2 = atan(y - h_mid, x) # atan2\n",
    "            r = sqrt(x^2 + (y - h_mid)^2)\n",
    "            \n",
    "            β2\n",
    "        end,\n",
    "        data\n",
    "    )\n",
    "    \n",
    "    sum(values)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surgical-freeware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  113.724 ms (8 allocations: 53.39 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10166.387038500514"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime imager(Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bigger-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.447 ms (4632 allocations: 17.87 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-5.789977393569645e6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime imager(CuArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-baking",
   "metadata": {},
   "source": [
    "That's looking more like it! However in doing this we have massively slowed down the CPU version over the initial implementation. But this is to be expected, and we can ammend the dispatch issues for CPU vs GPU later once we have squeezed performance.\n",
    "\n",
    "*NB*: there is a slight biasing in this implementation towards the GPU, since for the CPU we call\n",
    "```julia\n",
    "Array(collect(...))\n",
    "```\n",
    "This barely changes the speed of the CPU version (<10 ms), but does reduce allocations significantly. I implemented it as above for ease of interoperability, though really I should be using the multiple dispatch to achieve this.\n",
    "\n",
    "*Also NB*: we get GPU warnings about `atan` in `Base`. I'll investigate this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-cursor",
   "metadata": {},
   "source": [
    "## Reimplementing\n",
    "Let's reimplement the rendering function with just this change. I'm also going to take the liberty to slightly adjust the interface for multiple dispatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opponent-carolina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renderdisk! (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function renderdisk!(\n",
    "    α::Float64,\n",
    "    β::Float64,\n",
    "    geodesics,\n",
    "    data # no type to support recompilation on CuArray\n",
    "    ;\n",
    "    height::Int=720,\n",
    "    width::Int=1080,\n",
    "    fov_index::Int=200\n",
    "    )\n",
    "    \n",
    "    \n",
    "    h_mid = height ÷ 2\n",
    "    w_mid = width ÷ 2\n",
    "    \n",
    "    n = size(geodesics)[1] ÷ 3\n",
    "        \n",
    "    values = map(\n",
    "        (i) -> begin \n",
    "            # find position\n",
    "            z = (i-1) ÷ (width * height)\n",
    "            y = (i-1 - z*width*height) ÷ width\n",
    "            x = i - ( z*width*height + y * width ) - w_mid\n",
    "            y += 1\n",
    "            z += 1\n",
    "            \n",
    "            β2 = atan(y - h_mid, x) # atan2\n",
    "            r = sqrt(x^2 + (y - h_mid)^2)\n",
    "            \n",
    "            # additional steps here\n",
    "            index = convert(Int, floor(1 + r / fov_index * n))\n",
    "            \n",
    "            # we no longer assign directly, but return new value\n",
    "            ret = 0.0\n",
    "            if index < n\n",
    "                # has to calculate intersection three times, but this will be optimized later\n",
    "                ret = truncator(intersection(geodesics[index:index+2, :], α, β + β2))\n",
    "            end\n",
    "            ret\n",
    "        end,\n",
    "        data\n",
    "    )\n",
    "    \n",
    "    sum(values)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-williams",
   "metadata": {},
   "source": [
    "Let's now benchmark this new implementation, noting that, for comparison purposes, we also need to time the array creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "general-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  518.376 ms (376126 allocations: 2.64 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "837741.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test(geodesics, rinit::Type; height=720, width=1080)\n",
    "    data = reshape(\n",
    "        rinit(collect(Float64, 1:width*height*3)),\n",
    "        (height, width, 3)\n",
    "    )\n",
    "    \n",
    "    # squash so geodesics can be copied in CUDA.map call\n",
    "    geodesics = vcat(geodesics...)\n",
    "    \n",
    "    renderdisk!(π/100, 0.0, geodesics, data; height=height, width=width)\n",
    "end\n",
    "\n",
    "@btime test($geodesics, Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expanded-links",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#11#12\"{Int64, Int64, Int64, Float64, Float64, CuDeviceMatrix{Float64, 1}, Int64, Int64, Int64}, Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float64, 3, 1}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to print_to_string(xs...) in Base at strings/io.jl:124)\nStacktrace:\n  [1] \u001b[0m\u001b[1mstring\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./strings/\u001b[39m\u001b[90;4mio.jl:174\u001b[0m\n  [2] \u001b[0m\u001b[1mthrow_checksize_error\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:881\u001b[0m\n  [3] \u001b[0m\u001b[1m_unsafe_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:845\u001b[0m\n  [4] \u001b[0m\u001b[1m_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:832\u001b[0m\n  [5] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:1170\u001b[0m\n  [6] \u001b[0m\u001b[1m#11\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[13]:37\u001b[0m\n  [7] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:648\u001b[0m\n  [8] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:621\u001b[0m\n  [9] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:575\u001b[0m\n [10] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/8dzSJ/src/host/\u001b[39m\u001b[90;4mbroadcast.jl:59\u001b[0m\nReason: unsupported call through a literal pointer (call to )\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:450\u001b[0m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:458\u001b[0m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:750\u001b[0m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:740\u001b[0m\n  [5] \u001b[0m\u001b[1m_unsafe_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:844\u001b[0m\n  [6] \u001b[0m\u001b[1m_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:832\u001b[0m\n  [7] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:1170\u001b[0m\n  [8] \u001b[0m\u001b[1m#11\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[13]:37\u001b[0m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:648\u001b[0m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:621\u001b[0m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:575\u001b[0m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/8dzSJ/src/host/\u001b[39m\u001b[90;4mbroadcast.jl:59\u001b[0m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#11#12\"{Int64, Int64, Int64, Float64, Float64, CuDeviceMatrix{Float64, 1}, Int64, Int64, Int64}, Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float64, 3, 1}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to print_to_string(xs...) in Base at strings/io.jl:124)\nStacktrace:\n  [1] \u001b[0m\u001b[1mstring\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./strings/\u001b[39m\u001b[90;4mio.jl:174\u001b[0m\n  [2] \u001b[0m\u001b[1mthrow_checksize_error\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:881\u001b[0m\n  [3] \u001b[0m\u001b[1m_unsafe_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:845\u001b[0m\n  [4] \u001b[0m\u001b[1m_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:832\u001b[0m\n  [5] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:1170\u001b[0m\n  [6] \u001b[0m\u001b[1m#11\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[13]:37\u001b[0m\n  [7] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:648\u001b[0m\n  [8] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:621\u001b[0m\n  [9] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:575\u001b[0m\n [10] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/8dzSJ/src/host/\u001b[39m\u001b[90;4mbroadcast.jl:59\u001b[0m\nReason: unsupported call through a literal pointer (call to )\nStacktrace:\n  [1] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:450\u001b[0m\n  [2] \u001b[0m\u001b[1mArray\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:458\u001b[0m\n  [3] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:750\u001b[0m\n  [4] \u001b[0m\u001b[1msimilar\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:740\u001b[0m\n  [5] \u001b[0m\u001b[1m_unsafe_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:844\u001b[0m\n  [6] \u001b[0m\u001b[1m_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mmultidimensional.jl:832\u001b[0m\n  [7] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mabstractarray.jl:1170\u001b[0m\n  [8] \u001b[0m\u001b[1m#11\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[13]:37\u001b[0m\n  [9] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:648\u001b[0m\n [10] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:621\u001b[0m\n [11] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:575\u001b[0m\n [12] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/8dzSJ/src/host/\u001b[39m\u001b[90;4mbroadcast.jl:59\u001b[0m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#16\", Tuple{CUDA.CuKernelContext, CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#11#12\"{Int64, Int64, Int64, Float64, Float64, CuDeviceMatrix{Float64, 1}, Int64, Int64, Int64}, Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float64, 3, 1}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64}}}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/mjc8g/src/validation.jl:111",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/mjc8g/src/driver.jl:319 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/ZQ0rt/src/TimerOutput.jl:236 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/mjc8g/src/driver.jl:317 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/mjc8g/src/utils.jl:62",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/lwSps/src/compiler/execution.jl:317",
      "  [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/mjc8g/src/cache.jl:89",
      "  [8] cufunction(f::GPUArrays.var\"#broadcast_kernel#16\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#11#12\"{Int64, Int64, Int64, Float64, Float64, CuDeviceMatrix{Float64, 1}, Int64, Int64, Int64}, Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float64, 3, 1}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/lwSps/src/compiler/execution.jl:288",
      "  [9] cufunction(f::GPUArrays.var\"#broadcast_kernel#16\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceArray{Float64, 3, 1}, Base.Broadcast.Broadcasted{Nothing, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#11#12\"{Int64, Int64, Int64, Float64, Float64, CuDeviceMatrix{Float64, 1}, Int64, Int64, Int64}, Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float64, 3, 1}, Tuple{Bool, Bool, Bool}, Tuple{Int64, Int64, Int64}}}}, Int64}})",
      "    @ CUDA ~/.julia/packages/CUDA/lwSps/src/compiler/execution.jl:282",
      " [10] macro expansion",
      "    @ ~/.julia/packages/CUDA/lwSps/src/compiler/execution.jl:102 [inlined]",
      " [11] #launch_heuristic#241",
      "    @ ~/.julia/packages/CUDA/lwSps/src/gpuarrays.jl:17 [inlined]",
      " [12] launch_heuristic",
      "    @ ~/.julia/packages/CUDA/lwSps/src/gpuarrays.jl:17 [inlined]",
      " [13] copyto!",
      "    @ ~/.julia/packages/GPUArrays/8dzSJ/src/host/broadcast.jl:63 [inlined]",
      " [14] copyto!",
      "    @ ./broadcast.jl:936 [inlined]",
      " [15] copy",
      "    @ ~/.julia/packages/GPUArrays/8dzSJ/src/host/broadcast.jl:47 [inlined]",
      " [16] materialize(bc::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{3}, Nothing, var\"#11#12\"{Int64, Int64, Int64, Float64, Float64, CuArray{Float64, 2}, Int64, Int64, Int64}, Tuple{CuArray{Float64, 3}}})",
      "    @ Base.Broadcast ./broadcast.jl:883",
      " [17] map(::Function, ::CuArray{Float64, 3})",
      "    @ GPUArrays ~/.julia/packages/GPUArrays/8dzSJ/src/host/broadcast.jl:86",
      " [18] renderdisk!(α::Float64, β::Float64, geodesics::CuArray{Float64, 2}, data::CuArray{Float64, 3}; height::Int64, width::Int64, fov_index::Int64)",
      "    @ Main ./In[13]:18",
      " [19] test(geodesics::Vector{CuArray{Float64, 2}}, rinit::Type{CuArray}; height::Int64, width::Int64)",
      "    @ Main ./In[14]:10",
      " [20] test",
      "    @ ./In[14]:2 [inlined]",
      " [21] var\"##core#387\"(gpugeo#386::Vector{CuArray{Float64, 2}})",
      "    @ Main ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:479",
      " [22] var\"##sample#388\"(__params::BenchmarkTools.Parameters)",
      "    @ Main ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:485",
      " [23] _run(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; verbose::Bool, pad::String, kwargs::Base.Iterators.Pairs{Symbol, Integer, NTuple{4, Symbol}, NamedTuple{(:samples, :evals, :gctrial, :gcsample), Tuple{Int64, Int64, Bool, Bool}}})",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:98",
      " [24] #invokelatest#2",
      "    @ ./essentials.jl:710 [inlined]",
      " [25] #run_result#45",
      "    @ ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:33 [inlined]",
      " [26] run(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, kwargs::Base.Iterators.Pairs{Symbol, Integer, NTuple{5, Symbol}, NamedTuple{(:verbose, :samples, :evals, :gctrial, :gcsample), Tuple{Bool, Int64, Int64, Bool, Bool}}})",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:116",
      " [27] #warmup#54",
      "    @ ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:168 [inlined]",
      " [28] warmup(item::BenchmarkTools.Benchmark)",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:168",
      " [29] top-level scope",
      "    @ ~/.julia/packages/BenchmarkTools/tGTCy/src/execution.jl:565",
      " [30] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [31] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "@btime test($gpugeo, CuArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-cattle",
   "metadata": {},
   "source": [
    "This is a bit problematic. I obviously don't want a unique copy of `gpugeo` per GPU thread, and would like to put these arrays in read-only memory, so they can be accessed as needed for computational purposes. The alternative would be to pre-assemble the data required for each pixel in-place, but that would conflate the memory needs massively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-league",
   "metadata": {},
   "source": [
    "## Texture Memory\n",
    "I then stumbled accross an interesting section of the docs on [Texture Memory](https://juliagpu.github.io/CUDA.jl/stable/lib/driver/#CUDA.CuTexture-Tuple{Any}), which is interpolated read-only memory, nested around a CuArray, which I may be able to distribute over the threads:\n",
    "> Construct a N-dimensional texture object with elements of type T as stored in parent.\n",
    ">\n",
    ">Several keyword arguments alter the behavior of texture objects:\n",
    ">\n",
    ">    - address_mode (wrap, clamp, mirror): how out-of-bounds values are accessed. Can be specified as a value for all dimensions, or as a tuple of N entries.\n",
    ">\n",
    ">    - interpolation (nearest neighbour, linear, bilinear): how non-integral indices are fetched. Nearest-neighbour fetches a single value, others interpolate between multiple.\n",
    ">\n",
    ">    - normalized_coordinates (true, false): whether indices are expected to fall in the normalized [0:1) range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unknown-proof",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: CUDA does not support texture arrays for element type Float64.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: CUDA does not support texture arrays for element type Float64.",
      "",
      "Stacktrace:",
      " [1] convert(#unused#::Type{CUDA.CUarray_format_enum}, T::Type)",
      "   @ CUDA ~/.julia/packages/CUDA/lwSps/lib/cudadrv/memory.jl:800",
      " [2] alloc(#unused#::Type{CUDA.Mem.ArrayBuffer{Float64, N} where N}, dims::Tuple{Int64, Int64})",
      "   @ CUDA.Mem ~/.julia/packages/CUDA/lwSps/lib/cudadrv/memory.jl:319",
      " [3] CuTextureArray{Float64, 2}(#unused#::UndefInitializer, dims::Tuple{Int64, Int64})",
      "   @ CUDA ~/.julia/packages/CUDA/lwSps/src/texture.jl:40",
      " [4] CuTextureArray",
      "   @ ~/.julia/packages/CUDA/lwSps/src/texture.jl:91 [inlined]",
      " [5] CuTextureArray(A::CuArray{Float64, 2})",
      "   @ CUDA ~/.julia/packages/CUDA/lwSps/src/texture.jl:99",
      " [6] top-level scope",
      "   @ In[16]:1",
      " [7] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "CuTextureArray(gpugeo[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-thirty",
   "metadata": {},
   "source": [
    "But annoyingly we can't get Float64 support. We could cast it to Float32, which it does support, but I am increasingly unsure if these structures are what I want, amplified by the fact that the API is still experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-merchandise",
   "metadata": {},
   "source": [
    "## Memmory Management\n",
    "Examining the docs a little further, under their section on [Memmory Management](https://juliagpu.github.io/CUDA.jl/stable/lib/driver/#Memory-Management) are a few ways we can allocate and free memory on the GPU.\n",
    "\n",
    "From the docs, there are two functions which I think might solve my problem:\n",
    "\n",
    "> ```julia\n",
    "> Mem.alloc(DeviceBuffer, bytesize::Integer)\n",
    "> ```\n",
    ">\n",
    "> Allocate bytesize bytes of memory on the device. This memory is only accessible on the GPU, and requires explicit calls to `unsafe_copyto!`, which wraps `cuMemcpy`, for access on the CPU.\n",
    "\n",
    "and possibly\n",
    "\n",
    "> ```julia\n",
    "> Mem.register(HostBuffer, ptr::Ptr, bytesize::Integer, [flags])\n",
    "> ```\n",
    "> \n",
    "> Page-lock the host memory pointed to by `ptr`. Subsequent transfers to and from devices will be faster, and can be executed asynchronously. If the `HOSTREGISTER_DEVICEMAP` flag is specified, the buffer will also be accessible directly from the GPU. These accesses are direct, and go through the PCI bus. If the `HOSTREGISTER_PORTABLE` flag is specified, any CUDA context can access the memory.\n",
    "\n",
    "The second method would prevent having to upload, but would bottle neck at during reads. That being said, there are generally more pixels where no read is required, but I still am opting in favour of `alloc`.\n",
    "\n",
    "I'm thinking it's going to give me the speed, and I can use it without too much restriction, since I'm allocating a very small amount of memory. I think. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "undefined-cattle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4237033472"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.total_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "automotive-cause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeof(Float64) * sum(length, geodesics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-oklahoma",
   "metadata": {},
   "source": [
    "Yeah, that's shouldn't be a problem, with plently of space to scale vertically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-strap",
   "metadata": {},
   "source": [
    "### A single geodesic\n",
    "Let's see if we can allocate a single curve, and access it in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "twelve-newark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceBuffer(7.031 KiB at 0x0000000b01c58c00)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve = geodesics[1]\n",
    "curve_p = CUDA.Mem.alloc(\n",
    "    CUDA.Mem.DeviceBuffer,\n",
    "    sizeof(Float64) * length(curve)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-motorcycle",
   "metadata": {},
   "source": [
    "Going to make sure I understood the API for freeing also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "actual-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.Mem.free(curve_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-liquid",
   "metadata": {},
   "source": [
    "Yep, that works fine. \n",
    "\n",
    "Whilst trying to find more info on the `DeviceBuffer` API, I stumbled on a [few macros](https://juliagpu.github.io/CUDA.jl/stable/api/kernel/#Memory-types) which may also do what I want:\n",
    "\n",
    "> ```julia\n",
    "> @cuStaticSharedMem(T::Type, dims) -> CuDeviceArray{T,AS.Shared}\n",
    "> ```\n",
    "> \n",
    "> Get an array of type `T` and dimensions `dims` (either an integer length or tuple shape) pointing to a statically-allocated piece of shared memory. The type should be statically inferable and the dimensions should be constant, or an error will be thrown and the generator function will be called dynamically.\n",
    "\n",
    "Which, if I understand correctly should allow me to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "architectural-pontiac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×300 device array at Core.LLVMPtr{Float64, 3}(0x00007fa31539b1e0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve_mem = @cuStaticSharedMem Float64 (3, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-projection",
   "metadata": {},
   "source": [
    "Nice, it returns a proper `LLVMPtr`! Now we're in familiar territory. \n",
    "\n",
    "*NB*: I'm not entirely certain where this memory is located however... I get the suspicion this has been allocated on the host and shared with the device, but I am unsure how to check. \n",
    "\n",
    "Thankfully I'm not the first person to ponder this, and found [this NVIDIA developer page](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/) containing more information on what \"Shared Memory\" means in the context of GPUs.\n",
    "\n",
    "It is memory *allocated* on the GPU device, *shared* between threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "immediate-removal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isbits(curve_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-bailey",
   "metadata": {},
   "source": [
    "Perfect! So let's try this proof of concept with a conceptual function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "legal-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shared_test (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function shared_test(curve)\n",
    "    \n",
    "    curve_mem = @cuStaticSharedMem Float64 (3, 300)\n",
    "    curve_mem[:] = curve[:]\n",
    "    \n",
    "    data = CuArray(collect(Float32, 1:10000)) # something decently sized\n",
    "    map(\n",
    "        i -> begin\n",
    "            index = convert(Int, i % (3*300) + 1)\n",
    "            curve_mem[index]\n",
    "        end,\n",
    "        data\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "statewide-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared_test(curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-berry",
   "metadata": {},
   "source": [
    "This doesn't seem to work. I am encountering an `ERROR 700 ILLEGAL MEMORY ACCESS` which isn't boding well, and likewise I seem to be reading that shared memory is [significantly smaller](https://stackoverflow.com/a/20909276) than I'd at first hoped.\n",
    "\n",
    "Also, shared memory is access via kernels, not by the CPU."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
