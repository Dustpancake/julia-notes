{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63286d8",
   "metadata": {},
   "source": [
    "# A deep-dive into CUDA.jl\n",
    "\n",
    "Based on a [workshop from JuliaCon2021](https://www.youtube.com/watch?v=Hz9IMJuW5hU), link to [GitHub repository with the workshop notes](https://github.com/maleadt/juliacon21-gpu_workshop)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a0bec",
   "metadata": {},
   "source": [
    "A quick rule-of-thumb note on libraries:\n",
    "\n",
    "- NVIDIA uses [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)\n",
    "- AMD uses [AMDGPU.jl](https://github.com/JuliaGPU/AMDGPU.jl)\n",
    "- A common API is provided by [oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl)\n",
    "\n",
    "There are many other libraries available that wrap or leverage other specific features, but these may be considered as the main three. It is useful to bare in mind the differences when trying to write portable code.\n",
    "\n",
    "Another useful tool is [Tullio.jl](https://github.com/mcabbott/Tullio.jl), which provides einsum macros that automatically use GPU or threading features to speed up your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f4fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, BenchmarkTools, Tullio\n",
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb23ef",
   "metadata": {},
   "source": [
    "## Abstractions\n",
    "Although the Julia GPU libraries provide alternate implementations for many abstractions, they are not exhaustive. These implementations generate *scalar kernels* hidden behind the multiple dispatch, and are often simple drop in replacements when using standard algorithms or abstractions.\n",
    "\n",
    "Many are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff89126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.ones(5)\n",
    "broadcast(a) do x\n",
    "    x += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0ec44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a .+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cd4e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1}:\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0\n",
       " 2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(a) do x\n",
    "    x + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56c975c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0f0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(+, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280ca5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1}:\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulate(+, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1a415",
   "metadata": {},
   "source": [
    "However, CUDA is implementing these abstractions directly on type hierarchy and generating kernels. Thus their interoperability is not guarunteed, and using functions together operating on arrays will not result in GPU kernels being generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aafa212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float32}:\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = CUDA.ones(10, 10)\n",
    "broadcast(eachcol(b)) do x\n",
    "    sum(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f9ee9",
   "metadata": {},
   "source": [
    "The above only results in a single kernel launched per column. We can investigate this by examining the generated code using\n",
    "```julia\n",
    "@code_warntype\n",
    "```\n",
    "or\n",
    "```julia\n",
    "@code_typed\n",
    "```\n",
    "and indeed we see the broadcast being called is running on the CPU, with the `sum(x)` call spawing kernels.\n",
    "\n",
    "We can more intuitively see this in a benchmark comparison. We use\n",
    "```julia\n",
    "CUDA.@sync\n",
    "```\n",
    "to ensure the function call is synchronised with the execution on the device, as otherwise the function returns immediately after dispatching the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c940644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m199.408 μs\u001b[22m\u001b[39m … \u001b[35m 38.064 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 30.59%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m211.100 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m299.489 μs\u001b[22m\u001b[39m ± \u001b[32m875.649 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.66% ±  0.94%\n",
       "\n",
       "  \u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[32m▆\u001b[39m\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▇\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m█\u001b[39m \u001b[39m█\n",
       "  199 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        970 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m26.23 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m673\u001b[39m."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark CUDA.@sync broadcast(eachcol($b)) do x\n",
    "    sum(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f72da674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 13.833 μs\u001b[22m\u001b[39m … \u001b[35m  7.825 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 65.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m 15.171 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m344.530 μs\u001b[22m\u001b[39m ± \u001b[32m510.946 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.29% ±  0.96%\n",
       "\n",
       "  \u001b[34m█\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▄\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▆\u001b[39m \u001b[39m▃\u001b[39m \u001b[39m▁\n",
       "  \u001b[34m█\u001b[39m\u001b[39m▆\u001b[39m▄\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▄\u001b[39m█\u001b[39m▆\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▅\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m \u001b[39m█\n",
       "  13.8 μs\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       1.18 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m2.06 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m49\u001b[39m."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark CUDA.@sync sum(b; dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d5da8",
   "metadata": {},
   "source": [
    "More complex and involved kernels can also be generated by other tools, such as Tullio.jl. For example, the expression\n",
    "\n",
    "$$\n",
    "x_i = \\sum_{j} \\sum_{k} b_{ji} + b_{ik}\n",
    "$$\n",
    "\n",
    "can be executed as a GPU kernel with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea35dc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element CuArray{Int64, 1}:\n",
       "  5150\n",
       "  6250\n",
       "  7350\n",
       "  8450\n",
       "  9550\n",
       " 10650\n",
       " 11750\n",
       " 12850\n",
       " 13950\n",
       " 15050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = reshape(CUDA.CuArray(1:100), 10, 10)\n",
    "\n",
    "CUDA.@allowscalar @tullio x[i] := b[j, i] + b[i, k]"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
