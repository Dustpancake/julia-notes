{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Automatic Differention ([Automatic Differentiationin Machine Learning: a Survey](https://www.jmlr.org/papers/volume18/17-468/17-468.pdf) is a method of applying algorithmic differentiation, to compute derivatives on the fly by repeated application of the chain rule\n",
    "\n",
    "```{math}\n",
    "\\frac{\\text{d}}{\\text{d}x}f \\left( g \\left( x \\right) \\right) = \\frac{\\text{d}f}{\\text{d}g} \\frac{\\text{d}g}{\\text{d}x}.\n",
    "```\n",
    "\n",
    "For any given operation, we can draw a computational graph of operations, and thus calculate the total chain rule derivative, using the above formula. However, auto-diff can compute this for us using the concept of *Dual Numbers*, and corollaries of the chain rule (sum rule, product rule, quotient rule, sin, log, etc.).\n",
    "\n",
    "A [Dual Number](https://en.wikipedia.org/wiki/Dual_number) is the one-dimensional Grassman algebra over $\\mathbb{R}$. It is simply defined some $a, b$\n",
    "\n",
    "```{math}\n",
    "a + \\varepsilon b,\n",
    "```\n",
    "\n",
    "such that $\\varepsilon^2 = 0$.\n",
    "\n",
    "Heuristically, one can consider this as a value $a$, along with it's differential component $b$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A simple Julia implementation\n",
    "We define our dual number as a struct; here, let $x=a$ be the value of our number, and $\\epsilon = \\varepsilon b$ be it's derivative:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "struct D <: Number\n",
    "    x::Float64\n",
    "    ϵ::Float64\n",
    "end"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then define corollaries of the chain rule so that our Dual Number can be used:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import Base: +, -, *, /, sin, log, convert, promote_rule\n",
    "\n",
    "# summation rule\n",
    "a::D + b::D = D(a.x + b.x, a.ϵ + b.ϵ)\n",
    "a::D - b::D = D(a.x - b.x, a.ϵ - b.ϵ)\n",
    "\n",
    "# product rule\n",
    "a::D * b::D = D(a.x * b.x, (a.x * b.ϵ) + (a.ϵ * b.x) )\n",
    "\n",
    "# quotient rule\n",
    "a::D / b::D = D(a.x / b.x, ( (b.x * a.ϵ) - (a.x * b.ϵ) ) / (b.x^2))\n",
    "\n",
    "# sin & log\n",
    "sin(a::D) = D(sin(a.x), cos(a.x) * a.ϵ)\n",
    "log(a::D) = D(log(a.x), 1/a.x * a.ϵ)\n",
    "\n",
    "# conversion\n",
    "Base.convert(::Type{D}, x::Real) = D(x, zero(x))\n",
    "\n",
    "# always promote to Dual\n",
    "Base.promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can try it out on any function of choice: for some\n",
    "$$\n",
    "f(a, b) = \\log \\left( ab + \\sin(a) \\right),\n",
    "$$\n",
    "we want to compute\n",
    "$$\n",
    "\\frac{\\text{d}f}{\\text{d}a}.\n",
    "$$\n",
    "\n",
    "In Julia with our auto-diff:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "f(a, b) = log(a * b + sin(a))\n",
    "\n",
    "a = 3.1\n",
    "b = 2.4\n",
    "\n",
    "f(D(a, 1), b)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "D(2.0124440881688996, 0.18724182935843758)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note, we set $\\epsilon = 1$ as it represents $\\text{d}x / \\text{d}x$ in the chain rule expansion.\n",
    "\n",
    "Comparing to a symbolic result:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a)) # symbolic derivative\n",
    "\n",
    "f_derivative(a, b)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.18724182935843758"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "f(D(a,1), b).ϵ ≈ f_derivative(a, b)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can generalize the above for some impressive results, by adding a simple convenience function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "derivative(f::Function, x::Number) = f(D(x, one(x))).ϵ"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "derivative (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "derivative(x -> 3*x^2, 5) # -> 6 * x eval at x = 5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Forward and reverse mode\n",
    "\n",
    "The concept of *forward mode* and *reverse mode* is often discussed in auto-diff literature. A nice overview of the differences is given [by Mike Innes](https://github.com/MikeInnes/diff-zoo/blob/notebooks/backandforth.ipynb).\n",
    "\n",
    "We'll use packages for this later. In brief:\n",
    "\n",
    "- Forward mode: use for $\\mathbb{R} \\rightarrow \\mathbb{R}^n$\n",
    "- Reverse mode: use for $\\mathbb{R}^n \\rightarrow \\mathbb{R}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: Babylonian square root\n",
    "The [Babylonian square root](http://webhome.auburn.edu/~smith01/math3010Sp20/BabylonianSquareRoot.pdf) is an algorithm for calculating the square root of a number, up to a given repeated precision.\n",
    "\n",
    "In short, it is repeatedly applying\n",
    "$$\n",
    "t \\mapsto \\left( t + \\frac{1}{2} x \\right)\n",
    "$$\n",
    "until $t$ converges on $\\sqrt{x}$. In Julia:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "@inline function Babylonian(x; N=10)\n",
    "    t = (1 + x) / 2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t) / 2\n",
    "    end\n",
    "    t\n",
    "end"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Babylonian (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing the accuracy of the algorithm:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "Babylonian(2), √2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.414213562373095, 1.4142135623730951)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this implementation, we can obtain an implementation for the derivative for free:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "Babylonian(D(5, 1))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "D(2.23606797749979, 0.22360679774997896)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the LLVM optimizer uses parallelized instructions in the above (check with `@code_native`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward mode with ForwardDiff.jl\n",
    "\n",
    "Our above has been implemented much more extensively and complete in the [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) package (and associated [DiffRules.jl](https://github.com/JuliaDiff/DiffRules.jl)).\n",
    "\n",
    "ForwardDiff provides the concept of a *dual number* for us, as well as all the requesite chain rule applications. It is extremely simple to use:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "using ForwardDiff\n",
    "\n",
    "ForwardDiff.derivative(Babylonian, 5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.223606797749979"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "?ForwardDiff.derivative"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36m  ForwardDiff.derivative(f, x::Real)\u001b[39m\n",
       "\n",
       "  Return \u001b[36mdf/dx\u001b[39m evaluated at \u001b[36mx\u001b[39m, assuming \u001b[36mf\u001b[39m is called as \u001b[36mf(x)\u001b[39m.\n",
       "\n",
       "  This method assumes that \u001b[36misa(f(x), Union{Real,AbstractArray})\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  ForwardDiff.derivative(f!, y::AbstractArray, x::Real, cfg::DerivativeConfig = DerivativeConfig(f!, y, x), check=Val{true}())\u001b[39m\n",
       "\n",
       "  Return \u001b[36mdf!/dx\u001b[39m evaluated at \u001b[36mx\u001b[39m, assuming \u001b[36mf!\u001b[39m is called as \u001b[36mf!(y, x)\u001b[39m where the\n",
       "  result is stored in \u001b[36my\u001b[39m.\n",
       "\n",
       "  Set \u001b[36mcheck\u001b[39m to \u001b[36mVal{false}()\u001b[39m to disable tag checking. This can lead to\n",
       "  perturbation confusion, so should be used with care."
      ],
      "text/markdown": [
       "```\n",
       "ForwardDiff.derivative(f, x::Real)\n",
       "```\n",
       "\n",
       "Return `df/dx` evaluated at `x`, assuming `f` is called as `f(x)`.\n",
       "\n",
       "This method assumes that `isa(f(x), Union{Real,AbstractArray})`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "ForwardDiff.derivative(f!, y::AbstractArray, x::Real, cfg::DerivativeConfig = DerivativeConfig(f!, y, x), check=Val{true}())\n",
       "```\n",
       "\n",
       "Return `df!/dx` evaluated at `x`, assuming `f!` is called as `f!(y, x)` where the result is stored in `y`.\n",
       "\n",
       "Set `check` to `Val{false}()` to disable tag checking. This can lead to perturbation confusion, so should be used with care.\n"
      ],
      "text/latex": [
       "\\begin{verbatim}\n",
       "ForwardDiff.derivative(f, x::Real)\n",
       "\\end{verbatim}\n",
       "Return \\texttt{df/dx} evaluated at \\texttt{x}, assuming \\texttt{f} is called as \\texttt{f(x)}.\n",
       "\n",
       "This method assumes that \\texttt{isa(f(x), Union\\{Real,AbstractArray\\})}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "ForwardDiff.derivative(f!, y::AbstractArray, x::Real, cfg::DerivativeConfig = DerivativeConfig(f!, y, x), check=Val{true}())\n",
       "\\end{verbatim}\n",
       "Return \\texttt{df!/dx} evaluated at \\texttt{x}, assuming \\texttt{f!} is called as \\texttt{f!(y, x)} where the result is stored in \\texttt{y}.\n",
       "\n",
       "Set \\texttt{check} to \\texttt{Val\\{false\\}()} to disable tag checking. This can lead to perturbation confusion, so should be used with care.\n",
       "\n"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reverse mode with Zygote.jl\n",
    "\n",
    "An implementation of reverse mode auto-diff is in [Zygote.jl](https://github.com/FluxML/Zygote.jl), which is the basis of most of the Julia ML ecosystem.\n",
    "\n",
    "Similarly to ForwardDiff, Zygote is very straight forward to use:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "using Zygote\n",
    "\n",
    "gradient(Babylonian, 5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.22360679774997896,)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "?gradient"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "search: \u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36m  gradient(f, args...)\u001b[39m\n",
       "\n",
       "  Returns a tuple containing \u001b[36m∂f/∂x\u001b[39m for each argument \u001b[36mx\u001b[39m, the derivative (for\n",
       "  scalar x) or the gradient.\n",
       "\n",
       "  \u001b[36mf(args...)\u001b[39m must be a real number, see \u001b[36mjacobian\u001b[39m for array output."
      ],
      "text/markdown": [
       "```\n",
       "gradient(f, args...)\n",
       "```\n",
       "\n",
       "Returns a tuple containing `∂f/∂x` for each argument `x`, the derivative (for scalar x) or the gradient.\n",
       "\n",
       "`f(args...)` must be a real number, see [`jacobian`](@ref) for array output.\n"
      ],
      "text/latex": [
       "\\begin{verbatim}\n",
       "gradient(f, args...)\n",
       "\\end{verbatim}\n",
       "Returns a tuple containing \\texttt{∂f/∂x} for each argument \\texttt{x}, the derivative (for scalar x) or the gradient.\n",
       "\n",
       "\\texttt{f(args...)} must be a real number, see \\href{@ref}{\\texttt{jacobian}} for array output.\n",
       "\n"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}